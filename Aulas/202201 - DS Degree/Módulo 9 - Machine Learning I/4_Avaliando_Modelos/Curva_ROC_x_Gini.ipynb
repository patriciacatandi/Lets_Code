{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.4. Curva ROC\n",
    "Uma forma alternativa e comum de avaliar classificadores em problemas binários é por meio do uso das curvas ROC (Receiving Operating Characteristics).\n",
    "\n",
    "Seu gráfico é bidimensional: <br>\n",
    "        * eixo y -  True Positive Rate (TPR) = Recall <br>\n",
    "        * eixo x -  False Positive Rate (FPR) = (1 - Specificity) <br>\n",
    "\n",
    "Para um modelo genérico a curva ROC é parecida com a da figura a seguir:\n",
    "\n",
    "<img src=\"images/ROCAUC.png\"  style=\"width: 500px\" />\n",
    "\n",
    "Em resumo, a curva representa a relação entre a TPR e a FPR e quanto maior a **área abaixo da curva (AUC - Area Under Curve)** melhor é o modelo.\n",
    "<br>\n",
    "Para plotar essa curva nós precisamos setar diferentes valores de threshold que definem a qual classe pertence cada amostra e calcular qual o valor de TPR e FPR para esse threshold. Considere um problema de duas classes cada um com 100 amostras cujo gráfico de probabilidades se encontra abaixo. Selecionamos 7 thresholds distintos que irão definir à qual classe percence aquela amostra cujos valores estão exemplificados nas figuras.\n",
    "\n",
    "<img src=\"images/prob example.png\"  style=\"width: 1000px\" />\n",
    "\n",
    "\n",
    "| Threshold |                     Descrição                      | TPR = tp/tp+fn  | FPR = 1 - tn/fp+tn | \n",
    "|-----------|----------------------------------------------------|-----------------|-------------------|\n",
    "|     0     | Todas as amostras são classificadas como Positivas | 100/(100+0) = 1 | 1 - 0/(100+0) = 1 |\n",
    "|    100    | Todas as amostras são classificadas como Negativas | 0/(0+100) = 0   | 1 - 100/(0+100) = 0 |\n",
    "\n",
    "Idealmente queremos reduzir a quantidade de amostras classificadas incorretamente (FP e FN) e ao mesmo tempo tentar deixar a curva o mais próximo de TPR=1 e FPR=0 até que todas as amostras TP tenham sido classificadas. Dependendo da curva obtida com o modelo podemos classificá-lo de acordo com sua performance.\n",
    "\n",
    "<img src=\"images/ROCAUC2.png\"  style=\"width: 500px\" />\n",
    "\n",
    "Se um modelo se encontra na diagonal, dizemos que ele possui comportamento similar ao lançamento de uma moeda não viciada e temos um modelo aleatório.\n",
    "\n",
    "*Modelos abaixo dessa linha são piores que o aleatório, enquanto que acima são modelos melhores que o aleatório.*\n",
    "\n",
    "* Se um modelo está na **ponta superior esquerda** (chamada de céu ROC), dizemos que é um **modelo perfeito**;\n",
    "* Se está na **ponta superior direita ou inferior esqueda**, o modelo sempre classificará novos itens como positivos ou negativos, respectivamente;\n",
    "* Se está na **ponta inferior direita** (chamada de inferno ROC), esse modelo estará **sempre errando**.\n",
    "\n",
    "\n",
    "<img src=\"images/1341805045.jpg\" text=\"Fonte: www.datasciencecentral.com/roc-curve-explained-in-one-picture\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Comparando modelos distintos\n",
    "Com essa métrica também conseguimos comparar modelos distintos sendo o melhor deles aquele que se aproxima de TPR=1:\n",
    "\n",
    "<img src=\"images/compare_models.png\"  style=\"width: 500px\" />\n",
    "\n",
    "*Quando não há interseções entre as curvas de dois modelos, significa que o modelo que possui sua curva mais próxima do céu ROC é o que oferece melhor desempenho.*\n",
    "\n",
    "Ao existir cruzamentos, cada um terá um desempenho melhor que o outro de acordo com a região.\n",
    "\n",
    "## AUC-ROC (Area Under the ROC Curve)\n",
    "\n",
    "Entretanto, o mais comum é a determinação da **área abaixo da curva ROC (AUC-ROC)** para cada modelo e compará-los com essa medida única, que é compreendida entre 0 e 1:\n",
    "\n",
    "* Valores próximos de 1 são considerados os melhores;\n",
    "* Valores próximos a 0,5 são considerados aleatórios.\n",
    "<br>\n",
    "A AUC-ROC trás duas grandes vantagens:\n",
    "\n",
    "* Mede a qualidade da predição do modelo independente do threshold;\n",
    "* Robustez contra o desbalanceamento.\n",
    "\n",
    "Modelos com maior AUC conseguem discriminar melhor entre as classes.\n",
    "\n",
    "ROC AUC also tends to be dominated by the \"high FPR\" points. Depending on the application, these points may be the least relevant. Consider the case where the model is used to refer high-risk transactions to experts who will conduct further vetting. There may only be enough humans to assess 50 transactions per unit time; since the most highly-ranked transactions occur on the \"left hand\" size of the ROC curve by definition, this is also the region with the lowest area. So by looking at the whole AUC, you're optimistically biasing your results upwards, i.e. ROC AUC is buoyed by the observations \"to the right\" of the actual set of observations which humans will vet. (Illustration is simple. Draw a vertical line at FPR<0.5 on any ROC curve. The area to left is higher for all such vertical lines.) To avoid this, some people use partial ROC AUC, which has its own host of problems, chief among them that software implementations tend to assume that you're interested in truncation at some value of FPR. But in the case that you care about the top n transactions, this approach is obviously wrong because the top n transactions will happen at different FPR values for different classifiers. Standardization of partial AUC (to preserve the property that AUC < 0.5 is worse than random, 1 is perfect, 0 is worthless) incurs further difficulties.\n",
    "\n",
    "A AUC deve ser utilizada em classificações binárias. Para multi-classes podemos usar o one-to-rest AUC.\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia:\n",
    "[ROC-AUC](https://github.com/tunanottechnical/Jupyter-Workings/blob/master/Machine%20Learning%20in%20Mineral%20Exploration%20-%20Understanding%20Classification%20Evaluation%20Metrics.ipynb) <br>\n",
    "\n",
    "## Material de Aprofundamento\n",
    "[PR-Curve](https://www.kaggle.com/code/reighns/tutorial-on-machine-learning-metrics/notebook) <br>\n",
    "[Adjusting the classification threshold](https://www.ritchieng.com/machine-learning-evaluate-classification-model/) <br>\n",
    "[Gini](https://yassineelkhal.medium.com/confusion-matrix-auc-and-roc-curve-and-gini-clearly-explained-221788618eb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
