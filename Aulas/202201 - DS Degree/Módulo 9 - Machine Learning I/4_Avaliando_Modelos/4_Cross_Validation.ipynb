{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula - Avaliando modelos\n",
    "\n",
    "Vamos ver outras estratégias para tentar escolher modelos por algum tipo de previsão do nosso erro de generalização.\n",
    "\n",
    "- 1) Holdout\n",
    "- 2) Leave One Out Cross Validation (LOOCV)\n",
    "- 3) Cross Validation (CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Estratégia \"Holdout set\": Conjuntos de treino, validação e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos, no aprendizado de máquina nós temos alguns dados (__conjunto de treino__), e depois fazemos um experimento com uma amostra de dados que nunca vimos (__conjunto de teste__) para saber o quão bem o modelo consegue generalizar.\n",
    "\n",
    "Assim, temos o erro dentro do conjunto de treino, $E_{in}$, e o erro de generalização, pra dados daquele tipo fora desse conjunto, $E_{out}$. \n",
    "<br><br>\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/treino_teste.png\" width=500>\n",
    "</div>\n",
    "\n",
    "O problema é que, __se usarmos o conjunto de teste de qualquer forma para aprendizado, o erro que obtivermos nele deixa de refletir o erro de generalização__. \n",
    "\n",
    "Por exemplo, se treinarmos 3 modelos, e compararmos eles usando o conjunto de teste, o erro no teste não reflete mais o $E_{out}$.\n",
    "\n",
    "Outro exemplo são certas transformações dos nossos dados. Imagina que pegamos nossos dados, \"normalizamos\" eles (ou seja, pegamos nossas features e transformamos elas de forma que tenham um range de 0 a 1), e então fazemos a divisão entre conjunto de treino e conjunto de teste. Nesse caso, você já usou o conjunto de teste para \"aprender\" algo (para normalizar, a gente usa o maior valor da feature na tabela). Logo, sua medida de $E_{out}$ não vale mais. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que fazer então? Nós usamos o __conjunto de validação__ (ou _hold-out set_).\n",
    "<br><br>\n",
    "<div>\n",
    "    <img src=\"images/treino_validacao.png\" width=500>\n",
    "</div>\n",
    "\n",
    "Ao separar uma amostra (conjunto de validação), e usá-la apenas para seleção de modelos, nós assumimos que esse uso não afeta muito o erro. Então vamos dizer que o erro de validação, $E_{val}$, __aproxima de certa forma o erro de generalização__.\n",
    "\n",
    "Isso foi o que fizemos nas últimas aulas.\n",
    "\n",
    "Após validar e escolher o modelo, nós queremos sempre seguir adiante com o melhor modelo possível. Assim, para melhorar o modelo, nós aumentamos o número de dados para treinar o modelo final.\n",
    "\n",
    "Então aí sim, nós juntamos treino, validação e teste em uma única base, e treinamos o modelo final. Entende-se que os erros do nosso algoritmo só tendem a diminuir, quando fazemos isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Leave One Out Cross Validation (LOOCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós usamos, até agora, a estratégia de \"hold-out set\" (ou de conjunto de validação) para podermos comparar modelos.\n",
    "\n",
    "Porém, ela tem algumas fraquezas. Especificamente, a gente precisa de dados suficientes no conjunto de validação para tentar aproximar melhor o erro de generalização. Mas isso faz com que tenhamos cada vez menos dados de treino, para treinar o melhor modelo possível!\n",
    "\n",
    "Será que teria alguma forma de garantirmos uma boa validação, mas ainda tendo o máximo possível de dados pra treino? \n",
    "\n",
    "A resposta é __sim__. Entra em cena o __Leave One Out Cross Validation (LOOCV)__.\n",
    "<br><br>\n",
    "<div>\n",
    "    <img src=\"images/LOOCV.png\" width=600>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Ele funciona assim:\n",
    "- Nós tiramos 1 ponto dos dados de treino\n",
    "- Treinamos o modelo nos outros N-1 pontos\n",
    "- Avaliamos o modelo naquele ponto que tiramos\n",
    "- Repete esse procedimento para _todos_ os pontos da base de treino \n",
    "\n",
    "Embora o Scikit-Learn tenha ferramentas para usarmos o LOOCV, raramente esse método é aplicado na prática. O motivo disso é que ele é __muito custoso computacionalmente__, e esse custo aumenta cada vez mais quanto mais dados de treino tivermos.\n",
    "\n",
    "Nós não vamos testar esse método devido a isso, e vamos focar no seu primo mais econômico: a validação cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Cross Validation (CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: Material de aulas do prof. Sandro Saorin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O __Cross Validation (CV)__ (ou validação cruzada, em português) é uma técnica muito utilizada para estimar o erro de generalização do modelo. Ele é semelhante ao LOOCV, mas invés de separarmos apenas 1 ponto de cada vez, nós separamos um __bloco de pontos__.\n",
    "\n",
    "Com ela podemos testar a capacidade de generalização de um modelo. A técnica faz divisões na base de dados de treino e teste e permite treinar e validar seus dados em diversos grupos distintos. Dessa forma, conseguimos mensurar a flexibilidade ou capacidade de generalização de um modelo antes mesmo da chegada de novos dados.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=600>\n",
    "\n",
    "Vamos discutir o passo a passo do que está acontecendo:\n",
    "- Como sempre, primeiro temos que garantir que a validação não aconteça usando a base de teste.\n",
    "- A gente separa a nossa base de treino em diversas partes (o mais comum são 3, 5 ou 10).\n",
    "- Cada parte vai ser usada 1 vez como base de validação, para um treino realizado nas outras partes. (Na figura, por exemplo, temos 5 rodadas, e em cada rodada um bloco diferente é usado como validação, enquanto os outros 4 são usados para treino).\n",
    "- Calculamos as métricas de avaliação em cada uma dessas rodadas, para a base de validação.\n",
    "- Por fim, temos como resultado o score médio para o nosso modelo (tirando a média das métricas para cada parte usada como validação), ou seja, o quão bom ele está generalizando.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "O scikit-learn tem diversas ferramentas para aplicarmos o Cross Validation.\n",
    "\n",
    "Podemos testar diretamente o nosso modelo com o uso da função [__cross_val_score__](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html).\n",
    "\n",
    "Podemos usar também classes que controlam essas separações em _folds_, que são o [__K-Fold__](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) e o [__Stratified K-Fold__](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html?highlight=stratifiedkfold#sklearn.model_selection.StratifiedKFold).\n",
    "<br><br>\n",
    "\n",
    "A diferença entre as duas é semelhante ao que discutimos sobre usar ou não o parâmetro \"stratify\" da função \"train_test_split\". O __K-Fold__ faz uma quebra sem se preocupar com a distribuição das classes, enquanto o __Stratified K-Fold__ garante a mesma proporção em todos os _folds_.\n",
    "\n",
    "A gente pode usar o _stratified k-fold_ para quaisquer problemas que quisermos, embora o mais comum seja ele ser usado quando as nossas classes _target_ são muito __desbalanceadas__. Se uma das classes aparece muito menos que a outra, é possível que tenhamos uma quebra muito ruim. Em casos extremos, se uma classe aparece muito pouco, é possível que, sem a estratificação, ela sequer apareça em alguns _folds_.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Vamos ver como funciona a validação cruzada na prática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_ANgR1FTqXi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/bank-names.txt', 'r') as fp:\n",
    "    print(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVm7AWcyTqXn",
    "outputId": "7015e055-f46d-4eb6-9c0f-3ca62fbaba5c"
   },
   "outputs": [],
   "source": [
    "# Carregando o dataset e olhando para os nossos dados\n",
    "df = pd.read_csv('datasets/bank-full.csv', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos dar uma olhada em como está os nossos dados\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhofpm0GTqXx",
    "outputId": "aa2a0425-336d-4b23-eeb9-1586d09eb8f4"
   },
   "outputs": [],
   "source": [
    "# Verificando a proporção da target\n",
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgRJMALrTqXz"
   },
   "outputs": [],
   "source": [
    "# Dropando a target e a duração da chamada das nossas features\n",
    "X = df.drop(columns=['y', 'duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As nossas variáveis categóricas precisam ser transformadas em dados numéricos.\n",
    "Até agora, no máximo transformamos em um número de 0 a N.\n",
    "\n",
    "Contundo, geralmente essas variáveis não tem uma ordem. ('assalariado' é maior ou menor do que 'aposentado'?) Quando colocamos 0 a N, nós sem querer introduzimos essa ordem artificialmente.\n",
    "\n",
    "Assim, muitas vezes o ideal é quebrar aquela coluna em variáveis \"indicadoras\".\n",
    "\n",
    "Vão ser várias colunas, com valor 0 ou 1, sendo 0 quando não é aquele valor, e 1 quando é aquele valor. Chamamos isso de __One Hot Encoding__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bB8gN8wTqX1"
   },
   "outputs": [],
   "source": [
    "# Vamos ver um exemplo na prática.\n",
    "pontos = pd.DataFrame({'a':[1, 2, 1, 2, 1, 2],\n",
    "                       'b':[0.45, 0.1, 0.23, 0.98, 0.1, 0.999]})\n",
    "\n",
    "print(\"Como era nosso dataframe original:\")\n",
    "display(pontos)\n",
    "\n",
    "print(\"Como ele fica após fazermos One Hot Encoding:\")\n",
    "display(pd.get_dummies(pontos, prefix_sep='_', columns=['a']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O sklearn também tem uma ferramenta para realizar o mesmo procedimento, que é uma classe chamada \"OneHotEncoding\", mas não usaremos ele nessa aula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo um get_dummies para colunar as nossas variáveis categóricas\n",
    "X_with_dummies = pd.get_dummies(X, prefix_sep = '_', columns=['job', \n",
    "                                                              'marital', \n",
    "                                                              'education',\n",
    "                                                              'default',\n",
    "                                                              'housing',\n",
    "                                                              'loan',\n",
    "                                                              'contact',\n",
    "                                                              'month',\n",
    "                                                              'poutcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mvJfNGGTqX5",
    "outputId": "3c6abb1a-93a3-4b7b-bb8d-1adb25e485a2"
   },
   "outputs": [],
   "source": [
    "# transformando a target\n",
    "y_target = np.where(df['y'] == 'yes', 1, 0)\n",
    "y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NKz6wUPTqX9"
   },
   "outputs": [],
   "source": [
    "#Separando em train e test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with_dummies, \n",
    "                                                    y_target, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify = y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KD8yy_EQTqYC"
   },
   "outputs": [],
   "source": [
    "# Testando com o modelo DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEW7gELdTqYD"
   },
   "source": [
    "Vamos utilizar agora o \"cross_vasl_score\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W67qn7hpTqYD",
    "outputId": "6841bf8d-cad3-4061-8c35-c0cec4070192"
   },
   "outputs": [],
   "source": [
    "# Utilizando o Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(cross_val_score(model, X_train, y_train, scoring='accuracy', cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDKB3rSkTqYE"
   },
   "source": [
    "Agora testando o _Stratified K-Fold_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3hDg7MgTqYF",
    "outputId": "902ec989-8c7d-417e-936e-ee50588e2041"
   },
   "outputs": [],
   "source": [
    "# Utilizando o StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "kf.get_n_splits(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos transformar o X de treino em array do numpy para facilitar manipulação\n",
    "X_train_arr = X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTDv-cL8TqYH",
    "outputId": "6ba68f48-cddf-4b79-9464-cede0c2e2abc"
   },
   "outputs": [],
   "source": [
    "# Neste caso, teremos que montar manualmente cada iteração.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "list_accuracy = []\n",
    "list_precision = []\n",
    "list_recall = []\n",
    "list_f1_score = []\n",
    "\n",
    "i = 1\n",
    "for train_index, val_index in kf.split(X_train, y_train):\n",
    "    \n",
    "    print(\"============================================================================================\")\n",
    "    print(\"Fold \", i)\n",
    "    print(\"TRAIN:\", train_index, \"VALIDATION:\", val_index)\n",
    "    \n",
    "    # Pegando o X e o y de treino e de validação para aquela iteração\n",
    "    KFold_X_train, KFold_X_val = X_train_arr[train_index], X_train_arr[val_index]\n",
    "    KFold_y_train, KFold_y_val = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    # Treinando o modelo da iteração\n",
    "    model.fit(KFold_X_train, KFold_y_train)\n",
    "    \n",
    "    # Fazendo as previsões no \"fold\" de validação\n",
    "    y_pred = model.predict(KFold_X_val)\n",
    "    \n",
    "    #Calcula as métricas\n",
    "    acc = accuracy_score(KFold_y_val, y_pred)\n",
    "    prec = precision_score(KFold_y_val, y_pred)\n",
    "    recall = recall_score(KFold_y_val, y_pred)\n",
    "    f1 = f1_score(KFold_y_val, y_pred)\n",
    "    \n",
    "    # Mostrando em tela as métricas\n",
    "    print(\"Accuracy: \", acc)\n",
    "    print(\"Precison: \", prec)\n",
    "    print(\"Recal:    \", recall)\n",
    "    print(\"F1-Score: \", f1)\n",
    "    \n",
    "    # Salvando as métricas na lista\n",
    "    list_accuracy.append(acc)\n",
    "    list_precision.append(prec)\n",
    "    list_recall.append(recall)\n",
    "    list_f1_score.append(f1)\n",
    "    i += 1\n",
    "print(\"============================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "Para a próxima aula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1)__ Vamos repetir nosso exercício com o conjunto de dados \"Ames Housing\". \n",
    "Porém, desta vez, iremos comparar os modelos usando a validação cruzada.\n",
    "\n",
    "Como este é um caso de regressão, não faz sentido usarmos validação cruzada estratificada, então devemos usar a validação cruzada comum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DataScience)",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
